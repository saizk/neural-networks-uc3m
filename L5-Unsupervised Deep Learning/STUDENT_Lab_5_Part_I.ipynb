{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "STUDENT_Lab_5_Part_I.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-MQtQB2ytSY"
   },
   "source": [
    "# Lab 5 Part I: Image Autoencoder with CNNs\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "*Neural Networks. Bachelor in Data Science and Engineering*\n",
    "\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "*Aurora Cobo Aguilera acobo@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "In this notebook, we'll build a convolutional autoencoder to compress the CIFAR10 dataset. The encoder portion will be made of convolutional and pooling layers and the decoder will be made of **transpose convolutional layers** that learn to \"upsample\" a compressed representation.\n",
    "\n",
    "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-1Wr1zNwytSa"
   },
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://iq.opengenus.org/content/images/2019/03/autoencoder_1.png\", width=800, height=200)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BM0_AdntytSd"
   },
   "source": [
    "## Part I. Load CIFAR10. Visualize images\n",
    "\n",
    "Lets copy code that we've used in previous labs to load, normalize and visualize CIFAR10 images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "patpsIH5ytSd",
    "scrolled": true
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g6Gq0yKy7x5y"
   },
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
    "                                         shuffle=False, num_workers=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XevaXwoOkwfo"
   },
   "source": [
    "Lets visualize one image ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ypr0dwl-bOM"
   },
   "source": [
    "traindata = iter(trainloader)\n",
    "\n",
    "images, labels = next(traindata)\n",
    "\n",
    "print(images[1].shape)\n",
    "\n",
    "def rescale(img):\n",
    "    img = img / 2 + 0.5     # unnormalize to pot\n",
    "    npimg = img.numpy()\n",
    "    return np.transpose(npimg, (1, 2, 0))\n",
    "\n",
    "plt.imshow(rescale(images[0,:,:,:]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op4uNTcwk2AG"
   },
   "source": [
    "> **Exercise**: Check the range of every pixel in a normalized image. This will help you to select the appropiate activation function in the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ibvmvHpyLYSS"
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgfKJX7HytSh"
   },
   "source": [
    "> **Exercise:** Create a validation set with 5000 images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TIYbKKjm1x5C"
   },
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ruz27bZ0ytSm"
   },
   "source": [
    "---\n",
    "## Part II. Create the convolutional  autoencoder\n",
    "\n",
    "### Encoder\n",
    "The encoder part of the network will be a typical convolutional pyramid. Each convolutional layer \n",
    "will be followed by a max-pooling layer to reduce the dimensions of the layers. \n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder though might be something new to you. The decoder needs to convert from a narrow representation to a wide, reconstructed image. For example, the representation could be a 7x7x4 max-pool layer. This is the output of the encoder, but also the input to the decoder. We want to get a 28x28x1 image out from the decoder so we need to work our way back up from the compressed representation. A schematic of the network is shown below.\n",
    "\n",
    "**Note:** For CIFAR 10 we start with 32x32x3 images, and the encoder output will be of dimension 8x8xC, where C is the number of feature maps at the output. We will study the effect of C in reconstructing images. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yNLpg0Q4ytSm"
   },
   "source": [
    "Image(url= \"https://iq.opengenus.org/content/images/2019/03/autoencoder_3.png\", width=400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eg4RM0KytSo"
   },
   "source": [
    "\n",
    "In the MNIST example, the encoder layer has size 7x7x4 = 196. The original images have size 28x28 = 784, so the encoded vector is 25% the size of the original image. These are just suggested sizes for each of the layers. Feel free to change the depths and sizes, in fact, you're encouraged to add additional layers to make this representation even smaller! Remember our goal here is to find a small representation of the input data.\n",
    "\n",
    "### Transpose Convolutions, Decoder\n",
    "\n",
    "This decoder uses **transposed convolutional** layers to increase the width and height of the input layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3x3 kernel, a 3x3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a 3x3 path in a transposed convolution layer. PyTorch provides us with an easy way to create the layers, [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/nn.html#convtranspose2d). \n",
    "\n",
    "It is important to note that transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns. This is due to overlap in the kernels which can be avoided by setting the stride and kernel size equal. In [this Distill article](http://distill.pub/2016/deconv-checkerboard/) from Augustus Odena, *et al*, the authors show that these checkerboard artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer.  For simplicity, **we will put a convolution layer after the transpose convolutional layers to remove the artifacts.**\n",
    "\n",
    "> **Exercise:** Complete the following code, in which we build the autoencoder using a series of convolutional layers, pooling layers, and transpose convolutional layers. When building the decoder, recall that transpose convolutional layers can upsample an input by a factor of 2 using a stride and kernel_size of 2. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wc3hV4xzytSo"
   },
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self,C=4):\n",
    "        super().__init__()\n",
    "\n",
    "        #C is the number of feature maps of the encoder's output\n",
    "\n",
    "        self.n_channel_latent = C\n",
    "\n",
    "        print(\"The dimension of the latent representation is {0:f}\".format((32/4)**2*self.n_channel_latent))\n",
    "        \n",
    "        ## Encoder layers ##\n",
    "        # conv layer (depth from 3 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, \n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # conv layer (depth from 16 --> C), 3x3 kernels\n",
    "        # YOUR CODE HERE\n",
    "        self.conv2 = # YOUR CODE HERE\n",
    "        \n",
    "        # Max pool layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "\n",
    "        # trans conv layer (depth C --> 16). We increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(self.n_channel_latent, 16, kernel_size=2, stride=2)\n",
    "\n",
    "        # trans conv layer (depth 16 --> 16). We increase the spatial dims by 2\n",
    "        self.t_conv2 = # YOUR CODE HERE\n",
    "\n",
    "        # conv layer (depth 16 --> 3) no spatial reduction!!\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=3, \n",
    "                               kernel_size=1, stride=1, padding=0)        \n",
    "\n",
    "        ## BN Layers\n",
    "\n",
    "        # One BN layer after the first two convolutional layers\n",
    "\n",
    "        self.BN_1 = # YOUR CODE HERE\n",
    "\n",
    "        self.BN_2 = # YOUR CODE HERE\n",
    "\n",
    "        # One BN layer after the two transpose convolutional layers\n",
    "\n",
    "        self.BN_3 = # YOUR CODE HERE\n",
    "\n",
    "        self.BN_4 = # YOUR CODE HERE\n",
    "\n",
    "        # We use RELU activation for all layers\n",
    "        self.relu    = nn.ReLU()\n",
    "        \n",
    "        # And the appropiate activation at the decoder's output\n",
    "        self.decoder_activation = # YOUR CODE HERE\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "\n",
    "        # YOUR CODE HERE (many lines)        \n",
    "        \n",
    "\n",
    "        # x is input batch of images\n",
    "        # latent is the encoder output        \n",
    "        \n",
    "        return x,latent"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WuE4c5vc8tjt"
   },
   "source": [
    "# initialize the NN for C=16 layers\n",
    "model = ConvAutoencoder(C=16)\n",
    "print(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPyXwB08ytSq"
   },
   "source": [
    "---\n",
    "## Part III. Training\n",
    "\n",
    "> **Exercise:** Complete the class below to include a training method which monitors the reconstrution loss in both the training and validation datasets. In [this blog](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7) you can find a good summary of Pytorch Loss Functions. For the problem at hand, decide which one is more convenient. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NRjo5ZIOytSq"
   },
   "source": [
    "class ConvAutoencoder_extended(ConvAutoencoder):\n",
    "    \n",
    "    def __init__(self, epochs=10, lr=0.01,C=4):\n",
    "        \n",
    "        super().__init__(C)\n",
    "        self.lr = lr    \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)   \n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.criterion = # YOUR CODE HERE\n",
    "\n",
    "        self.loss_during_training = []\n",
    "        self.valid_loss_during_training = []\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            running_loss = 0.\n",
    "            start_time = time.time()\n",
    "            for images, labels in trainloader:\n",
    "        \n",
    "                labels = images # To train an Autoencoder the label is the input\n",
    "\n",
    "                # Move input and label tensors to the default device \n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "            \n",
    "                out = self.forward(images)[0]\n",
    "\n",
    "                loss =  self.criterion(out, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                self.optim.step()\n",
    "                   \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "            self.valid_loss_during_training.append(self.eval_performance(validloader))\n",
    "                \n",
    "\n",
    "            if(e % 1 == 0): # Every epoch\n",
    "\n",
    "                print(\"Epoch %d. Training loss: %f, Validation loss: %f, Time per epoch: %f seconds\" \n",
    "                      %(e,self.loss_during_training[-1],self.valid_loss_during_training[-1],\n",
    "                       (time.time() - start_time)))\n",
    "\n",
    "    def eval_performance(self,dataloader):\n",
    "\n",
    "        # YOUR CODE HERE (many lines)\n",
    "                \n",
    "\n",
    "        "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm1bw-v0ytSs"
   },
   "source": [
    "> **Exercise:** Train for 5 epochs the model for 16 feature maps at the output of the encoder (C=16) and plot both training and validation losses. Can you visualize overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QgoktnHYytSs"
   },
   "source": [
    "# Your code here\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KvPi2gPLytSu"
   },
   "source": [
    "# Your code here\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUmaIFnHytSw"
   },
   "source": [
    "### Checking out the results\n",
    "\n",
    "Below we plot some of the **test images** along with their reconstructions. These look a little rough around the edges, likely due to the artifacts that tend to happen with transpose layers.\n",
    "\n",
    "> **Exercise:** Complete the code (just one line!)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ybVmIyd7ytSw"
   },
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "batch_size=256\n",
    "\n",
    "images_cuda, labels_cuda = images.to(autoencoder.device), labels.to(autoencoder.device)\n",
    "\n",
    "# latent representation for the minibtach\n",
    "output,latent = # YOUR CODE HERE\n",
    "\n",
    "# prep images for display\n",
    "\n",
    "\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 3, 32, 32)\n",
    "# use detach when it's an output that requires_grad. We use .cpu() to move the result back to cpu from gpu\n",
    "output = output.cpu().detach()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for i in range(10):\n",
    "    axes[0,i].imshow(rescale(images[i,:,:,:]))\n",
    "    axes[1,i].imshow(rescale(output[i,:,:,:]))\n",
    "\n",
    "    axes[0,i].get_xaxis().set_visible(False) # Remove legend\n",
    "    axes[0,i].get_yaxis().set_visible(False)\n",
    "\n",
    "    axes[1,i].get_xaxis().set_visible(False) # Remove legend\n",
    "    axes[1,i].get_yaxis().set_visible(False)    \n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENTPVtxptFpv"
   },
   "source": [
    "## Part IV. Visualizing the effect of the encoder size\n",
    "\n",
    "To analyze the effect of C, the number of feature maps at the encoder's output, represent the validation loss after training 5 epochs our autoencoder for different values of C between 2 and 32."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0U7FERF1uJmf"
   },
   "source": [
    "# Your code here"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re4MOuFd64ue"
   },
   "source": [
    "## Part V. Visualize the data the encoder's output in 2D using t-SNE\n",
    "\n",
    "[t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 100 or 200 variables) if the number of features is very high. At the end, this is what we used the autoencoder for!\n",
    "\n",
    "> **Exercise** For C=4, compute the latent representation of a mini-batch of CIFAR 10 images. Dont forget to move the latent representation to the cpu (in case a gpu was used)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CRKnKpYDvE9Y"
   },
   "source": [
    "# YOUR CODE HERE"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7aoMfsNwiLY"
   },
   "source": [
    "We implement TSNE using sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ef30JEXB5sRK"
   },
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reshape latent representation to a single vector\n",
    "latent = latent_cpu.reshape(batch_size,-1)\n",
    "images_np = np.squeeze(images)\n",
    "\n",
    "latent_tsne = TSNE(n_components=2).fit_transform(latent)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hz3qexwxGxLo"
   },
   "source": [
    "# With this code we can visualize images in a 2D scatter plot\n",
    "\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    " \n",
    "def plot_latent_space_with_images(images,latent,xmin=-1,xmax=1,ymin=-1,ymax=1):\n",
    " \n",
    "    # images --> Minibatch of images (numpy array!)\n",
    "    # latent --> Matrix of 2D representations (numpy array!)\n",
    " \n",
    "    f, ax = plt.subplots(1,1,figsize=(8, 8))\n",
    "    # ax is a figure handle\n",
    "    ax.clear()\n",
    "    for i in range(len(images)):\n",
    "        im = OffsetImage(rescale(images[i,:,:,:]))\n",
    "        ab = AnnotationBbox(im, latent[i,:],frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "    #We set the limits according to the maximum and minimum values found for the latent projections\n",
    "    ax.set_xlim(xmin,xmax)\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "    ax.set_title('Latent space Z with Images')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AORMX4Vj6SHT"
   },
   "source": [
    "plot_latent_space_with_images(images, latent_tsne,-10,10,-10,10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLmxFVJxEKq"
   },
   "source": [
    "Do the neighbours in the 2D space make any sense?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xi7r5Tsd6dVt"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P2-HRErMWnqu"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}