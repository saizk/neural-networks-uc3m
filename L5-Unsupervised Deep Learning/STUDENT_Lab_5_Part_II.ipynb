{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"STUDENT_Lab_5_Part_II.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bB3CCfaL2_6Q"},"source":["# Lab 5 Part 2: Understanding Variational Autoencoders (VAE)\n","\n","\n","\n","------------------------------------------------------\n","*Neural Networks. Bachelor in Data Science and Engineering*\n","\n","*Pablo M. Olmos pamartin@ing.uc3m.es*\n","\n","*Aurora Cobo Aguilera acobo@tsc.uc3m.es*\n","\n","------------------------------------------------------\n","\n","In this notebook, you will complete the code to train a VAE for a dataset of images of celebrities: [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). We will use a reduced set of 11.000 images to speed things up.\n","\n","\n","### Probabilistic model\n","\n","The probabilistic model to fit to the data is exactly the same one seen in class. If $\\mathbf{x}^{(i)}$ are the training images and $\\mathbf{z}^{(i)}$ are the corresponding latent (unobserved) embeddings of dimension $k$, $i=1,\\ldots,11.000$. Then ...\n","\n","\\begin{align}\n","p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{0},\\mathbf{I})\\\\\n","p(\\mathbf{x}|\\mathbf{z}) &= \\mathcal{N}(\\mu_\\theta(\\mathbf{z}),\\sigma_x\\mathbf{I})\n","\\end{align}\n","where $\\mu_\\theta(\\mathbf{z})$ is a Neural Network with input $\\mathbf{z}$ and $\\sigma_x$ is a reconstruction noise power that we won't train. By default, we will take the value $\\sigma_x=0.1$. \n","\n","### Inference model\n","\n","We will approximate the posterior distribution $p(\\mathbf{z}|\\mathbf{x})$ by the following parametric model\n","\n","$$p(\\mathbf{z}|\\mathbf{x})\\approx q(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mu_\\eta(\\mathbf{x}),\\text{diag}(\\sigma_\\eta(\\mathbf{x}))$$\n","\n","where both $\\mu_\\eta(\\mathbf{x}),\\text{diag}(\\sigma_\\eta(\\mathbf{x})$ are neural networks with input $\\mathbf{x}$.\n","\n","### Training loss: the evidence lower bound (ELBO)\n","\n","Since exact inference is intractable in the former model, we will train the parameters of the probabilistic and inference models using the ELBO lower bound\n","\n","$$\\log p(\\mathbf{x}) \\geq \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\left[\\log p(\\mathbf{x}|\\mathbf{z})\\right] - D_{KL}(q(\\mathbf{z}|\\mathbf{x}||p(\\mathbf{z}))$$\n","\n","Since $p(\\mathbf{z})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and $q(\\mathbf{z}|\\mathbf{x})$ are both Gaussian then\n","\n","$$D_{KL}(q(\\mathbf{z}|\\mathbf{x}||p(\\mathbf{z})) = \\frac{1}{2}\\left(-k+\\sum_{j=1}^k \\sigma_{\\eta,j}-\\log(\\sigma_{\\eta,j})+\\mu_{\\eta,j}^2\\right)$$\n","\n","During training, the first expectation will be replaced by a one-sample Monte Carlo estimate for every data point. Overall, the ELBO to be optimized using SGD is\n","\n","$$\\text{ELBO}(\\theta,\\eta,\\sigma_x) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\log p(\\mathbf{x}^{(i)}|\\tilde{\\mathbf{z}}^{(i)}) - D_{KL}(q(\\mathbf{z}|\\mathbf{x}^{(i)}||p(\\mathbf{z}))\\right)$$\n","\n","where $\\tilde{\\mathbf{z}}^{(i)}$ is a sample from $q(\\mathbf{z}|\\mathbf{x}^{(i)})$ computed as\n","\n","$$\\tilde{\\mathbf{z}}^{(i)} =  \\mu_\\eta(\\mathbf{x}) + \\sqrt{\\sigma_\\eta(\\mathbf{x})} \\epsilon$$\n","and $\\epsilon$ is a sample from a k-dimensional standard Gaussian distribution. This is the **reparameterization trick**. Note that samples are differentiable w.r.t. the parameters in both $\\mu_\\eta(\\mathbf{x})$ and $\\sigma_\\eta(\\mathbf{x})$."]},{"cell_type":"code","metadata":{"id":"6eQChN-ouFDl"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","import torchvision\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch import optim\n","import time\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_qA1nEo7Ajmo"},"source":["## Part I: Loading the reduced CelebA dataset\n","\n","First, import to your own Drive the folder I shared with you in Aula Global."]},{"cell_type":"code","metadata":{"id":"NAof8Ck3udy6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UtXlzf3wujSu"},"source":["path_to_folder = '/content/drive/My Drive/DL_Colab_2020/reduced_celebA/' # UPDATE THIS ACCORDING TO YOUR PATH!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ocxkzwdnBcem"},"source":["With the following function we import the database using Pytorch dataloaders ..."]},{"cell_type":"code","metadata":{"id":"mMf8KN88uFDm"},"source":["def get_celeba(batch_size, dataset_directory):\n","\n","    train_transformation = transforms.Compose([\n","        transforms.Resize((64, 64)),        # THE IMAGES ARE RE-SCALED TO 64x64\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    train_dataset = torchvision.datasets.ImageFolder(dataset_directory + 'celeba', train_transformation)\n","\n","\n","    # Use sampler for randomization\n","    training_sampler = torch.utils.data.SubsetRandomSampler(range(len(train_dataset)))\n","\n","    # Prepare Data Loaders for training and validation\n","    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=training_sampler)\n","\n","    return trainloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQQfg3koBsqV"},"source":["# We load the database in a single trainloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2RvRmKSuFDm"},"source":["trainloader = get_celeba(32,path_to_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHKEyKaEB2Kh"},"source":["In the following code, we read one mini-batch of images and plot some of them."]},{"cell_type":"code","metadata":{"id":"a-iwMzzmuFDm"},"source":["def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize to pot\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwPRmrBMuFDo"},"source":["train = iter(trainloader)\n","\n","i,l = next(train)\n","\n","imshow(i[0,:,:,:])\n","\n","imshow(torchvision.utils.make_grid(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vD6EyJhgCR7O"},"source":["## A class to implement the encoder mean and variance\n","\n","With the following class, we implement the CNN network to obtain both $\\mu_\\eta(\\mathbf{x}), \\sigma_\\eta(\\mathbf{x})$, which determine the moments of the approximate posterior distribution $q(\\mathbf{z}|\\mathbf{x})$.\n","\n","We use a single CNN with input $\\mathbf{x}$, the output vector of dimension $2k$ is divided in two parts. The first $k$ elements represent the mean, and we compute the variance from the last $k$ elements using a soft-plus.\n","\n","The actual design of this network is borrowed from [this paper](https://paperswithcode.com/method/beta-vae). It could be improved by adding batch norm layers, but you will see it does perform well already.\n","\n","> **Exercise**: Complete the following code. What are the spatial dimensions of the 256 feature maps at the ouput of the `conv5` convolutional layer? "]},{"cell_type":"code","metadata":{"id":"vum--eXHuFDo"},"source":["class encoder(nn.Module):\n","    \n","    def __init__(self,dimz,channels=3,var_x=0.1):\n","        \n","        \n","        super().__init__()\n","        \n","        self.dimz = dimz    #dimz is k, the dimension of the latent space\n","\n","        # self.conv1 is a convolutional layer, with 32 output channels, kernel size 4, stride 2,\n","        # and padding 1\n","        \n","        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, \n","                               kernel_size=4, stride=2, padding=1)\n","        \n","        self.relu = nn.ReLU()\n","        \n","        # self.conv2 is a convolutional layer, with 32 output channels, kernel size 4, stride 2,\n","        # and padding 1\n","\n","        self.conv2 = #YOUR CODE HERE\n","\n","        # self.conv3 is a convolutional layer, with 64 output channels, kernel size 4, stride 2,\n","        # and padding 1  \n","        \n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, \n","                               kernel_size=4, stride=2, padding=1)      \n","\n","        # self.conv4 is a convolutional layer, with 64 output channels, kernel size 4, stride 2,\n","        # and padding 1  \n","\n","        self.conv4 = #YOUR CODE HERE\n","\n","        # self.conv5 is a convolutional layer, with 256 output channels, kernel size 4, stride 1,\n","        # and padding 0                                \n","        \n","        self.conv5 = nn.Conv2d(in_channels=64, out_channels=256, \n","                               kernel_size=4, stride=1, padding=0) \n","        \n","\n","        # self.linear is a linear layer with dimz*2 outputs\n","        self.linear = #YOUR CODE HERE\n","        \n","        self.softplus = nn.Softplus()\n","        \n","    def forward(self,x):\n","\n","        # The forward method to project and image into a 2dimz dimensional vector\n","        \n","        z = self.relu(self.conv1(x))\n","        z = #YOUR CODE HERE\n","        z = self.relu(self.conv3(z))\n","        z = #YOUR CODE HERE\n","        z = self.relu(self.conv5(z))\n","        # Transform z into a 256-dim vector\n","        z = #YOUR CODE HERE\n","        z = self.linear(z)\n","        \n","        return z\n","    \n","    def encode_and_sample(self,x,flag_sample=True):\n","\n","        # This methods compute both the posterior mean and variance\n","        # Also we obtain a sample from the posterior using the\n","        # reparameterization trick.\n","\n","        # We obtain the encoder projection using the forward method\n","        \n","        z = #YOUR CODE HERE\n","\n","        # The mean is the first dimz components of the forward output\n","        \n","        mu = #YOUR CODE HERE\n","\n","        # We compute the variance from the last dimz components using a \n","        # soft plus\n","        var = self.softplus(0.5 * z[:, self.dimz:])\n","        \n","        sample = None\n","        \n","        if(flag_sample==True):\n","            \n","            eps = torch.randn_like(var) \n","            \n","            sample = mu + eps*(var**0.5)\n","        \n","        return mu,var,sample\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LHlky1wlvz2"},"source":["> **Exercise**: Using `dimz=2`, create an encoder object and print the mean `mu_z` and variance `var z` in the z space for one image."]},{"cell_type":"code","metadata":{"id":"wc7OpZvml9IO"},"source":["#YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIvHTxAJi5OP"},"source":["## A class to implement the decoder mean\n","\n","Given a sample from $\\mathbf{z}$, we generate images by sample from $p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}),\\sigma_x\\mathbf{I})$, where we take $\\sigma_x=0.1$ as a reconstruction noise. The following class implements the mean NN $\\mu_\\theta(\\mathbf{z})$ using transpose convolutions. Again, the network design comes from the same [this paper](https://paperswithcode.com/method/beta-vae).\n","\n","> **Exercise**: complete the following code. Why do we use an hiperbolic tangent output activation?\n"]},{"cell_type":"code","metadata":{"id":"8wGgbnavuFDo"},"source":["class decoder(nn.Module):\n","    \n","    def __init__(self,dimz,channels=3,var_x=0.1):\n","        \n","        super().__init__()\n","\n","        # We expand z into a 256 dimensional vector \n","        \n","        self.linear = nn.Linear(dimz,256)\n","        \n","        self.relu = nn.ReLU()\n","\n","        self.tanh = nn.Tanh()\n","\n","        # self.tconv1 is a convolutional layer, with 64 output channels, kernel size 4, stride 1,\n","        # and padding 0 \n","        \n","        self.tconv1 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=1,padding=0)\n","\n","        # self.tconv2 is a convolutional layer, with 64 output channels, kernel size 4, stride 2,\n","        # and padding 1         \n","        \n","        self.tconv2 = #YOUR CODE HERE\n","\n","        # self.tconv3 is a convolutional layer, with 32 output channels, kernel size 4, stride 2,\n","        # and padding 1  \n","        \n","        self.tconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2,padding=1)\n","\n","        # self.tconv3 is a convolutional layer, with 32 output channels, kernel size 4, stride 2,\n","        # and padding 1\n","        \n","        self.tconv4 = #YOUR CODE HERE\n","\n","        # self.tconv3 is a convolutional layer, with channels output channels, kernel size 4, stride 2,\n","        # and padding 1        \n","        \n","        self.tconv5 = nn.ConvTranspose2d(32, channels, kernel_size=4, stride=2,padding=1)\n","        \n","    def forward(self,z):\n","\n","        x = self.relu(self.linear(z).view(-1,256,1,1))\n","        x = #YOUR CODE HERE\n","        x = self.relu(self.tconv2(x))\n","        x = #YOUR CODE HERE\n","        x = self.relu(self.tconv4(x))\n","        x = #YOUR CODE HERE   \n","        return x\n","    \n","    def decode(self,z):\n","\n","        # This function simply calls the forward method\n","        \n","        return self.forward(z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tV5DyU1RoMWt"},"source":["> **Exercise:** create a decoder object using again `dimz=2`. Given the mean projection of an image `mu_z` that you obtained before, use the decoder to obtain the mean `x_mean` of $p(\\mathbf{x}|\\mathbf{z})$. Represent one original image versus the reconstruction. Obviously, since you have not trained the model yet, do expect too much."]},{"cell_type":"code","metadata":{"id":"9JRf2XGMoL5j"},"source":["my_dec = decoder(dimz=2)\n","\n","x_mean = my_dec.decode(mu_z).detach()\n","\n","imshow(i[0,:,:,:])\n","\n","imshow(x_mean[0,:,:,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FE-tHFQppwLb"},"source":["Observe that, since the mean `x_mean` is so different from the real image, then the real image is **extremely unlikely** in the distribution $p(\\mathbf{x}|\\mathbf{z})$. The model will never generate it by sampling. \n","\n","For one image `x`, the following function evaluates the log-likelihood of an independent Gaussian distribution given the mean and the diagonal covariance matrix. In the function, both `x` and `mu_x` are passed as images and internally stacked to vectors. `var_x` is a constant. Recall that `var_x` is a constant vector of 0.1 elements.\n","\n"]},{"cell_type":"code","metadata":{"id":"XErAjGJMpoAe"},"source":["def eval_Gaussian_LL(x,mu_x,var_x):\n","\n","    # x is a mini-batch of images. It has dimension [Batch,3,dimx,dimx]\n","\n","    # mu_x is a mini-batch of reconstructed images. It has dimension [Batch,3,dimx,dimx]\n","\n","    # var_x is a torch constant\n","\n","    D = x.shape[1] * x.shape[2] * x.shape[3]   # Dimension of the image\n","\n","    x = x.reshape(-1, D)\n","\n","    mu_x = mu_x.reshape(-1, D)\n","\n","    var_x = torch.ones_like(mu_x) * var_x\n","\n","    # Constant term in the gaussian distribution\n","    cnt = D * np.log(2 * np.pi) + torch.sum(torch.log(var_x), dim=-1)\n","\n","    # log-likelihood per datapoint\n","\n","    logp_data = -0.5 * (cnt + torch.sum((x - mu_x) * var_x ** -1 * (x - mu_x), dim=-1))\n","    \n","    # Accumulated Gaussian log-likelihood for all datapoints in the batch\n","    logp = torch.sum(logp_data)\n","\n","    return logp,logp_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3mfqqgsmqn5v"},"source":["> **Exercise:** Compute the log-likelihood of one real image given `x_mean` computed above and `var_x=0.1`."]},{"cell_type":"code","metadata":{"id":"cV94bU5lqmef"},"source":["var_x = 0.1\n","\n","logp,logp_data = #YOUR CODE HERE\n","\n","print(logp)\n","\n","plt.plot(np.arange(0,32),logp_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eO969bjWlTRd"},"source":["## The variational autoencoder class\n","\n","The following class puts together the VAE encoder & decoder and also defines the ELBO lower bound. We will extend it later to incorporate training methods.\n","\n","> **Exercise**: complete the following code. "]},{"cell_type":"code","metadata":{"id":"MHkxP9_UuFDo"},"source":["class VAE(nn.Module):\n","    \n","    def __init__(self,dimz,channels=3,var_x=0.1):\n","        \n","        super().__init__()\n","        \n","        self.var_x = var_x\n","        \n","        self.dimz = dimz\n","\n","        # We create an encoder network\n","        \n","        self.encoder = #YOUR CODE HERE\n","\n","        # We create a decoder network\n","        \n","        self.decoder = #YOUR CODE HERE\n","        \n","    def forward(self,x):\n","\n","        # In the forward method, we return the mean and variance \n","        # given by the encoder network and also the reconstruction mean\n","        # given by the decoder network using a sample from the \n","        # encoder's posterior distribution.\n","        \n","        mu_z,var_z,sample_z = #YOUR CODE HERE\n","        \n","        # Decoder provides the mean of the reconstruction\n","        \n","        mu_x = self.decoder.decode(sample_z) \n","        \n","        return mu_x,mu_z,var_z\n","    \n","    # Reconstruction + KL divergence losses summed over all elements and batch\n","\n","    def loss_function(self, x, mu_x, mu_z, var_z):\n","\n","        # We evaluate the loglikelihood in the batch using the function provided above\n","\n","        logp,_ = #YOUR CODE HERE\n","\n","        # KL divergence between q(z) and N()\n","        # see Appendix B from VAE paper:\n","        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","        # https://arxiv.org/abs/1312.6114\n","\n","        KLz = -0.5 * torch.sum(1 + torch.log(var_z) - mu_z.pow(2) - var_z)\n","\n","        # To maximize ELBO we minimize loss (-ELBO)\n","        return -logp + KLz, -logp, KLz  \n","    \n","\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tc9D0Uwydc5"},"source":["> **Exercise:** Create a VAE object for `dimz=2` and evaluate the ELBO using `mu_z`, `var_z`, and `x_mean` computed above."]},{"cell_type":"code","metadata":{"id":"yL59B29lvbYJ"},"source":["my_vae = #YOUR CODE HERE\n","\n","print(my_vae.loss_function(i,x_mean,mu_z,var_z))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTCAv5mpy5wi"},"source":["## Incorporating a training method\n","\n","The following class completes the implementation of the VAE. **You do not have to edit anything**, but it is highly recommended to go through the code. It is actually very similar to the training classes we have been implementing. Only three points to remark:\n","\n","\n","- Since training is pretty slow (every epoch can take a few minutes), we save the model every few minibatches. User indicates the saving path using the `save_folder` argument.\n","- I have introduced a `restore` flag. When set it to true, we load the model parameters from a file saved in the `save_folder` argument.\n","- The class also incorporates a method to sample from the generative model. Namely, create new images. To this end, we sample $\\mathbf{z}$ from $p(\\mathbf{z}) = \\mathcal{N}(0,I)$ and then we return the mean of $p(\\mathbf{x}|\\mathbf{z})$."]},{"cell_type":"code","metadata":{"id":"E4u3eCsDuFDo"},"source":["class VAE_extended(VAE):\n","\n","    def __init__(self, dimz=2,  channels=3, var_x=0.1,lr=1e-3,epochs=20,save_folder='./',restore=False):\n","        \n","        super().__init__(dimz,channels=3,var_x=0.1)\n","\n","        self.lr = lr    \n","        self.optim = optim.Adam(self.parameters(), self.lr)   \n","        self.epochs = epochs\n","\n","        self.save_folder = save_folder\n","\n","        if(restore==True):\n","            state_dict = torch.load(self.save_folder+'VAE_checkpoint.pth')\n","            self.load_state_dict(state_dict)\n","\n","        self.loss_during_training = []\n","        self.reconstruc_during_training = []\n","        self.KL_during_training = []\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.to(self.device)\n","\n","    def trainloop(self,trainloader):\n","\n","        nims = len(trainloader.dataset)\n","\n","        self.train()\n","\n","        for e in range(int(self.epochs)):\n","\n","            train_loss = 0\n","            train_rec = 0\n","            train_kl_l = 0\n","\n","            idx_batch = 0\n","\n","            for images, labels in trainloader:\n","\n","                images = images.to(self.device)\n","\n","                self.optim.zero_grad()\n","\n","                mu_x, mu_z, var_z = self.forward(images)\n","\n","                loss, rec, kl_l = self.loss_function(images,mu_x, mu_z, var_z)\n","\n","                loss.backward()\n","\n","                train_loss += loss.item()\n","                train_rec += rec.item()\n","                train_kl_l += kl_l.item() \n","\n","                self.optim.step()\n","\n","                if(idx_batch%10==0):\n","\n","                    torch.save(self.state_dict(), self.save_folder + 'VAE_checkpoint.pth')\n","\n","                idx_batch += 1\n","\n","            self.loss_during_training.append(train_loss/len(trainloader))\n","            self.reconstruc_during_training.append(train_rec/len(trainloader))\n","            self.KL_during_training.append(train_kl_l/len(trainloader))\n","\n","            if(e%1==0):\n","\n","                torch.save(self.state_dict(), self.save_folder + 'VAE_checkpoint.pth')\n","                print('Train Epoch: {} \\tLoss: {:.6f}'.format(e,self.loss_during_training[-1]))\n","\n","\n","    def sample(self,num_imgs):\n","\n","        with torch.no_grad():\n","\n","            eps = torch.randn([num_imgs,self.dimz]).to(self.device)\n","\n","            x_sample = self.decoder.decode(eps)\n","\n","            return x_sample.to(\"cpu\").detach()    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZtgqWMJf1Q78"},"source":["# Validate a pre-trained model\n","\n","First, we're going to see a VAE model in action. In the file `VAE_checkpoint.pth` linked in Aula Global, I give you the parameters of a VAE with `dim_z=50` trained for 200 epochs with `var_x=0.1`, `lr=1e-3`.\n","\n","> **Exercise**: Create a VAE with the parameters contained in the file `VAE_checkpoint.pth`. To do so, instantiate the class `VAE_extended` with the `restore_flag` set to `True`."]},{"cell_type":"code","metadata":{"id":"XzHw1AWRuFDo"},"source":["path_to_save = '/content/drive/My Drive/DL_Colab_2020/' # UPDATE THIS ACCORDING TO YOUR PATH!\n","my_vae = #YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-GxOMNU918kQ"},"source":["> **Exercise:** Using the method `sample` from the clase `VAE_extended`, generate 20 images from the probabilistic model. **Note these are images created by our model! They correspond to people that do not exist!**.\n","\n","There are many variants of VAEs that quite improve these results, but even this vanilla VAE we have implemented does a pretty god job! Despite the images contain lots of artifacts, note that the images contain lots of very realistic details. Also note how the model tries to vary the features of the images (hair, background, smiles etc ...). "]},{"cell_type":"code","metadata":{"id":"NOyWdmFe9JBD"},"source":["x = #YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"op2XwYlM9QwO"},"source":["imshow(x[0,:,:,:])\n","\n","imshow(torchvision.utils.make_grid(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31ZsINkQ2mPR"},"source":["> **Exercise:** Compare a real image versus its reconstruction. Note that the model is by default save in a GPU if available, so you have to move the images there. To visualize reconstructed images, you have to move the result of the decoder back to CPU."]},{"cell_type":"code","metadata":{"id":"y_H0QV8i2ldV"},"source":["z_batch,_,_ = my_vae.encoder.encode_and_sample(# YOUR CODE HERE) \n","\n","x_reconstructed = my_vae.decoder.decode(z_batch).detach() \n","\n","# We plot he original image\n","# YOUR CODE HERE\n","\n","# And the reconstruction\n","# YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zqpI1b_V4n--"},"source":["> Using the VAE, we can also *interpolate* between images. Namely, given the latent representation $\\mathbf{z}_1$ and $\\mathbf{z}_2$ of two images, we can visualize the images that correspond to different linear interpolations between both latent points. To this end, given $w=[0,0.1,0.2, ...,1]$, will visualize the mean of $p(\\mathbf{x}|\\mathbf{z})$ for \n","\n","$$z = (1-w)\\mathbf{z}_1+w\\mathbf{z}_2$$\n","\n","**Exercise:** Complete the following code to interpolate between two images"]},{"cell_type":"code","metadata":{"id":"Rt1m2JXq72Kv"},"source":["# Lets visualize the images to interpolate\n","\n","img_1 = i[0,:,:,:]  \n","img_2 = i[1,:,:,:]\n","\n","imshow(img_1)\n","\n","imshow(img_2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9n8ZhQVSlST"},"source":["# z_1 and z_2 are saved in z_batch, computed in the previous exercise\n","\n","z_1 = #YOUR CODE HERE\n","z_2 = #YOUR CODE HERE\n","\n","# Linear interpolation\n","\n","weight = torch.Tensor(1.0-np.arange(0,10,1)/10)\n","\n","reconstructed_image = [my_vae.decoder.decode(z_1*w+z_2*(1-w)).to('cpu').detach() for w in weight]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDmhaDkAYcH-"},"source":["# Visualize all images in the reconstructed_image list\n","\n","for im in reconstructed_image:\n","  #YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99oKEUV68Uso"},"source":[">**Exercise:** Using TSNE over `z_batch``, show how real images are projected in a 2D space. The following function is handy (it was already used in the previous lab)"]},{"cell_type":"code","metadata":{"id":"xmd0rtn28pAC"},"source":["from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","\n","def rescale(img):\n","    img = img / 2 + 0.5     # unnormalize to pot\n","    npimg = img.numpy()\n","    return np.transpose(npimg, (1, 2, 0))\n"," \n","def plot_latent_space_with_images(images,latent,xmin=-150,xmax=150,ymin=-150,ymax=150):\n"," \n","    # images --> Minibatch of images (numpy array!)\n","    # latent --> Matrix of 2D representations (numpy array!)\n"," \n","    f, ax = plt.subplots(1,1,figsize=(8, 8))\n","    # ax is a figure handle\n","    ax.clear()\n","    for i in range(len(images)):\n","        im = OffsetImage(rescale(images[i,:,:,:]))\n","        ab = AnnotationBbox(im, latent[i,:],frameon=False)\n","        ax.add_artist(ab)\n","    #We set the limits according to the maximum and minimum values found for the latent projections\n","    ax.set_xlim(xmin,xmax)\n","    ax.set_ylim(ymin,ymax)\n","    ax.set_title('Latent space Z with Images')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTHiuW0U8qMf"},"source":["from sklearn.manifold import TSNE\n","\n","# Apply TSNE over z_batch\n","\n","latent_tsne = TSNE(n_components=2).fit_transform(#YOUR CODE HERE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTM8k5Eb86hX"},"source":["plot_latent_space_with_images(i, latent_tsne)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1kZqmS96izB"},"source":["## Train the VAE from scratch (OPTIONAL)\n","\n","Just to finish, train your own VAE from zero for a few epochs and generate images from the model. "]},{"cell_type":"code","metadata":{"id":"BxZ4nYd-6h9Z"},"source":[" #YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTFfmqxx7GWI"},"source":[""],"execution_count":null,"outputs":[]}]}